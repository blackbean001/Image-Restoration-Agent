Setting ds_accelerator to cuda (auto detect)
weights/vicuna-7b-v1.5/
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  4.64s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.01s/it]
/opt/conda/envs/depictqa/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/opt/conda/envs/depictqa/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
trainable params: 16777216 || all params: 6755192832 || trainable%: 0.24836028248556738
root
â”œâ”€â”€ vision_encoder (VisionTransformer) class_embedding:[1024] 
â”‚   positional_embedding:[257, 1024] proj:[1024, 768]
â”‚   â”œâ”€â”€ conv1 (Conv2d) weight:[1024, 3, 14, 14]
â”‚   â”œâ”€â”€ ln_pre,ln_post(LayerNorm) weight:[1024] bias:[1024]
â”‚   â””â”€â”€ transformer (Transformer)
â”‚       â””â”€â”€ resblocks (Sequential)
â”‚           â””â”€â”€ 0-23(ResidualAttentionBlock)
â”‚               â”œâ”€â”€ attn (MultiheadAttention) in_proj_weight:[3072, 1024] 
â”‚               â”‚   in_proj_bias:[3072]
â”‚               â”‚   â””â”€â”€ out_proj (NonDynamicallyQuantizableLinear) weight:[1024,
â”‚               â”‚       1024] bias:[1024]
â”‚               â”œâ”€â”€ ln_1,ln_2(LayerNorm) weight:[1024] bias:[1024]
â”‚               â””â”€â”€ mlp (Sequential)
â”‚                   â”œâ”€â”€ c_fc (Linear) weight:[4096, 1024] bias:[4096]
â”‚                   â””â”€â”€ c_proj (Linear) weight:[1024, 4096] bias:[1024]
â”œâ”€â”€ llm (PeftModelForCausalLM)
â”‚   â””â”€â”€ base_model (LoraModel)
â”‚       â””â”€â”€ model (LlamaForCausalLM)
â”‚           â”œâ”€â”€ model (LlamaModel)
â”‚           â”‚   â”œâ”€â”€ embed_tokens (Embedding) weight:[32000, 4096]
â”‚           â”‚   â”œâ”€â”€ layers (ModuleList)
â”‚           â”‚   â”‚   â””â”€â”€ 0-31(LlamaDecoderLayer)
â”‚           â”‚   â”‚       â”œâ”€â”€ self_attn (LlamaAttention)
â”‚           â”‚   â”‚       â”‚   â””â”€â”€ q_proj,k_proj,v_proj,o_proj(Linear) 
â”‚           â”‚   â”‚       â”‚       weight:[4096, 4096]
â”‚           â”‚   â”‚       â”‚       â”œâ”€â”€ lora_dropout (ModuleDict)
â”‚           â”‚   â”‚       â”‚       â”œâ”€â”€ lora_A (ModuleDict)
â”‚           â”‚   â”‚       â”‚       â”‚   â””â”€â”€ default (Linear) weight:[16, 4096]
â”‚           â”‚   â”‚       â”‚       â””â”€â”€ lora_B (ModuleDict)
â”‚           â”‚   â”‚       â”‚           â””â”€â”€ default (Linear) weight:[4096, 16]
â”‚           â”‚   â”‚       â”œâ”€â”€ mlp (LlamaMLP)
â”‚           â”‚   â”‚       â”‚   â”œâ”€â”€ gate_proj,up_proj(Linear) weight:[11008, 4096]
â”‚           â”‚   â”‚       â”‚   â””â”€â”€ down_proj (Linear) weight:[4096, 11008]
â”‚           â”‚   â”‚       â””â”€â”€ input_layernorm,post_attention_layernorm(LlamaRMSNor
â”‚           â”‚   â”‚           m) weight:[4096]
â”‚           â”‚   â””â”€â”€ norm (LlamaRMSNorm) weight:[4096]
â”‚           â””â”€â”€ lm_head (Linear) weight:[32000, 4096]
â”œâ”€â”€ abstractor (AbstractorModel) query_embeds:[64, 1, 1024]
â”‚   â””â”€â”€ abstractor (TransformerDecoder)
â”‚       â””â”€â”€ layers (ModuleList)
â”‚           â””â”€â”€ 0-3(TransformerDecoderLayer)
â”‚               â”œâ”€â”€ self_attn,multihead_attn(MultiheadAttention) 
â”‚               â”‚   in_proj_weight:[3072, 1024] in_proj_bias:[3072]
â”‚               â”‚   â””â”€â”€ out_proj (NonDynamicallyQuantizableLinear) weight:[1024,
â”‚               â”‚       1024] bias:[1024]
â”‚               â”œâ”€â”€ linear1 (Linear) weight:[2048, 1024] bias:[2048]
â”‚               â”œâ”€â”€ linear2 (Linear) weight:[1024, 2048] bias:[1024]
â”‚               â””â”€â”€ norm1,norm2,norm3(LayerNorm) weight:[1024] bias:[1024]
â””â”€â”€ vision_proj (Linear) weight:[4096, 1024] bias:[4096]
 * Serving Flask app 'app_comp'
 * Debug mode: on
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5002
 * Running on http://172.17.0.2:5002
INFO:werkzeug:[33mPress CTRL+C to quit[0m
